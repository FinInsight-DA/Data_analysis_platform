# -*- coding: utf-8 -*-
"""
LDA í† í”½ ëª¨ë¸ë§ ìë™í™” (ë¡œì»¬ ë²„ì „)
"""

import time
import os
import pickle
import hashlib
import json
from pathlib import Path

import pandas as pd
import numpy as np
import re
from tqdm import tqdm
from konlpy.tag import Okt
from gensim import corpora
from gensim.models import LdaModel, CoherenceModel

# ============================================================================
# ì„¤ì •
# ============================================================================
INPUT_CSV = '/Users/song/Desktop/workspace/fin/hv_labeled.csv'  # ì…ë ¥ CSV íŒŒì¼ ê²½ë¡œ
OUTPUT_DIR = './LDA_results'         # ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
CACHE_DIR = './LDA_cache'            # ìºì‹œ ë””ë ‰í† ë¦¬

# í•™ìŠµí•  í† í”½ ê°œìˆ˜ ë¦¬ìŠ¤íŠ¸
TOPIC_NUMBERS = [5, 10, 15, 20]

# LDA í•˜ì´í¼íŒŒë¼ë¯¸í„°
PASSES = 5
ITERATIONS = 50
ALPHA = 'auto'  # 'auto', 'symmetric', 'asymmetric' ë˜ëŠ” ìˆ«ìê°’
ETA = 'auto'    # 'auto', 'symmetric' ë˜ëŠ” ìˆ«ìê°’

# Dictionary í•„í„°ë§ íŒŒë¼ë¯¸í„°
NO_BELOW = 5      # ìµœì†Œ ë¬¸ì„œ ë¹ˆë„
NO_ABOVE = 0.5    # ìµœëŒ€ ë¬¸ì„œ ë¹„ìœ¨
KEEP_N = 1000     # ìµœëŒ€ ë‹¨ì–´ ìˆ˜

# ì „ì²˜ë¦¬ íŒŒë¼ë¯¸í„°
MIN_NOUN_LENGTH = 2  # ìµœì†Œ ëª…ì‚¬ ê¸¸ì´

# ë¶ˆìš©ì–´
STOP_WORDS = {
    'ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì˜', 'ì™€', 'ê³¼', 'ë„',
    'ì—', 'ë¡œ', 'ì—ì„œ', 'ë¶€í„°', 'ê¹Œì§€',
    'í•˜ë‹¤', 'ìˆë‹¤', 'ë˜ë‹¤', 'ê°™ë‹¤', 'ì—†ë‹¤',
    'ê²ƒ', 'ìˆ˜', 'ë“±', 'ê°œ', 'ëª…', 'ë…„', 'ì›”', 'ì¼',
    'ì—…ê³„', 'ê¸°ì—…', 'íšŒì‚¬', 'ì—…ì²´', 'ê´€ê³„ì',
    'ì˜¬í•´', 'ë‚´ë…„', 'ì‘ë…„', 'ì´ë²ˆ', 'ì§€ë‚œí•´', 'ìµœê·¼'
}

# ë””ë ‰í† ë¦¬ ìƒì„±
os.makedirs(CACHE_DIR, exist_ok=True)
os.makedirs(OUTPUT_DIR, exist_ok=True)

# ============================================================================
# LDA í´ë˜ìŠ¤
# ============================================================================
class LDATopicModeling:
    """LDA í† í”½ ëª¨ë¸ë§ ìë™í™”"""
    
    def __init__(self, df, stop_words, min_noun_length=2, verbose=True):
        self.df = df
        self.stop_words = stop_words
        self.min_noun_length = min_noun_length
        self.verbose = verbose
        self.processed_sentences = None
        self.dictionary = None
        self.corpus = None
        self.models = {}
        self.topics_dict = {}
        self.coherence_scores = {}
        self.perplexity_scores = {}
    
    def preprocess(self, use_cache=True):
        """í˜•íƒœì†Œ ë¶„ì„ (ìºì‹œ í™œìš©)"""
        # ìºì‹œ íŒŒì¼ëª… ìƒì„±
        data_hash = hashlib.md5(
            (self.df['sentence'].str.cat() + str(self.min_noun_length)).encode()
        ).hexdigest()[:8]
        cache_file = f"{CACHE_DIR}/processed_{data_hash}.pkl"
        
        # ìºì‹œ ë¡œë“œ
        if use_cache and os.path.exists(cache_file):
            if self.verbose:
                print("ğŸ“¦ ìºì‹œëœ ì „ì²˜ë¦¬ ê²°ê³¼ ë¡œë“œ ì¤‘...")
            with open(cache_file, 'rb') as f:
                self.processed_sentences = pickle.load(f)
            if self.verbose:
                print(f"âœ… í˜•íƒœì†Œ ë¶„ì„ ì™„ë£Œ (ìºì‹œ): {len(self.processed_sentences):,}ê°œ ë¬¸ì¥")
            return
        
        # í˜•íƒœì†Œ ë¶„ì„
        if self.verbose:
            print("\nğŸ“ í˜•íƒœì†Œ ë¶„ì„ ì‹œì‘...")
        
        okt = Okt()
        
        def clean_text(text):
            if pd.isna(text):
                return []
            text = re.sub(r'[^ê°€-í£a-zA-Z0-9\s]', '', str(text))
            try:
                nouns = okt.nouns(text)
                return [
                    n for n in nouns
                    if len(n) >= self.min_noun_length
                    and not n.isdigit()
                    and n not in self.stop_words
                ]
            except:
                return []
        
        self.processed_sentences = []
        for text in tqdm(self.df['sentence'], desc="í˜•íƒœì†Œ ë¶„ì„", disable=not self.verbose):
            self.processed_sentences.append(clean_text(text))
        
        self.processed_sentences = [s for s in self.processed_sentences if len(s) > 0]
        
        # ìºì‹œ ì €ì¥
        with open(cache_file, 'wb') as f:
            pickle.dump(self.processed_sentences, f)
        
        if self.verbose:
            print(f"âœ… í˜•íƒœì†Œ ë¶„ì„ ì™„ë£Œ: {len(self.processed_sentences):,}ê°œ ë¬¸ì¥")
    
    def create_dict_corpus(self, no_below, no_above, keep_n):
        """Dictionary & Corpus ìƒì„±"""
        if self.verbose:
            print("\nğŸ“š Dictionary & Corpus ìƒì„± ì¤‘...")
        
        self.dictionary = corpora.Dictionary(self.processed_sentences)
        original_size = len(self.dictionary)
        
        self.dictionary.filter_extremes(
            no_below=no_below,
            no_above=no_above,
            keep_n=keep_n
        )
        
        self.corpus = [self.dictionary.doc2bow(text) for text in self.processed_sentences]
        
        if self.verbose:
            print(f"âœ… Dictionary & Corpus ìƒì„± ì™„ë£Œ")
            print(f"   - ì›ë³¸ ë‹¨ì–´ ìˆ˜: {original_size:,}")
            print(f"   - í•„í„°ë§ í›„: {len(self.dictionary):,}")
            print(f"   - Corpus í¬ê¸°: {len(self.corpus):,}")
        
        return original_size, len(self.dictionary)
    
    def train_lda(self, n_topics, passes, iterations, alpha, eta):
        """LDA í•™ìŠµ"""
        if self.verbose:
            print(f"\nğŸš€ LDA í•™ìŠµ ì‹œì‘ ({n_topics}ê°œ í† í”½)...")
            print(f"   - Passes: {passes}")
            print(f"   - Iterations: {iterations}")
            print(f"   - Alpha: {alpha}")
            print(f"   - Eta: {eta}")
        
        start_time = time.time()
        
        model = LdaModel(
            corpus=self.corpus,
            id2word=self.dictionary,
            num_topics=n_topics,
            passes=passes,
            iterations=iterations,
            random_state=42,
            per_word_topics=False,
            alpha=alpha,
            eta=eta
        )
        
        self.models[n_topics] = model
        
        # í† í”½ í• ë‹¹
        doc_topics = []
        for bow in self.corpus:
            topic_dist = model.get_document_topics(bow)
            if topic_dist:
                dominant = max(topic_dist, key=lambda x: x[1])[0]
                doc_topics.append(dominant)
            else:
                doc_topics.append(-1)
        
        self.topics_dict[n_topics] = np.array(doc_topics)
        
        # Coherence ê³„ì‚°
        if self.verbose:
            print("   ğŸ“Š Coherence ê³„ì‚° ì¤‘...")
        
        coherence_model = CoherenceModel(
            model=model,
            texts=self.processed_sentences,
            dictionary=self.dictionary,
            coherence='c_v',
            processes=1  # macOS multiprocessing ì—ëŸ¬ ë°©ì§€
        )
        coherence = coherence_model.get_coherence()
        self.coherence_scores[n_topics] = coherence
        
        # Perplexity ê³„ì‚°
        perplexity = model.log_perplexity(self.corpus)
        self.perplexity_scores[n_topics] = perplexity
        
        elapsed = time.time() - start_time
        
        if self.verbose:
            print(f"âœ… í•™ìŠµ ì™„ë£Œ! (ì†Œìš” ì‹œê°„: {elapsed:.1f}ì´ˆ)")
            print(f"   - Coherence: {coherence:.4f}")
            print(f"   - Perplexity: {perplexity:.2f}")
            print(f"   - í• ë‹¹ëœ ë¬¸ì„œ: {(self.topics_dict[n_topics] != -1).sum():,}ê°œ")
        
        return coherence, perplexity
    
    def get_result_df(self, n_topics):
        """ê²°ê³¼ ë°ì´í„°í”„ë ˆì„ ìƒì„±"""
        topics = self.topics_dict[n_topics]
        result_df = self.df.iloc[:len(topics)].copy()
        result_df['lda_topic'] = topics
        result_df = result_df[result_df['lda_topic'] != -1]
        return result_df
    
    def save_results(self, n_topics, selected_topics=None):
        """ê²°ê³¼ ì €ì¥"""
        if self.verbose:
            print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘ ({n_topics}ê°œ í† í”½)...")
        
        model = self.models[n_topics]
        result_df = self.get_result_df(n_topics)
        
        # ì„ íƒí•œ í† í”½ë§Œ í•„í„°ë§
        if selected_topics is not None:
            result_df = result_df[result_df['lda_topic'].isin(selected_topics)].copy()
            suffix = f"_selected_{len(selected_topics)}topics"
        else:
            suffix = ""
        
        timestamp = pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')
        
        # CSV ì €ì¥
        output_csv = f"{OUTPUT_DIR}/lda_{n_topics}_topics{suffix}_{timestamp}.csv"
        result_df.to_csv(output_csv, index=False, encoding='utf-8-sig')
        
        # Excel ì €ì¥ (í‚¤ì›Œë“œ í¬í•¨)
        output_excel = f"{OUTPUT_DIR}/lda_{n_topics}_topics{suffix}_{timestamp}.xlsx"
        
        # í† í”½ë³„ í‚¤ì›Œë“œ í…Œì´ë¸” ìƒì„±
        keywords_data = []
        for topic_id in range(n_topics):
            words = model.show_topic(topic_id, topn=10)
            keywords = ', '.join([f"{word}({prob:.3f})" for word, prob in words[:5]])
            keywords_data.append({
                'í† í”½': f"Topic {topic_id}",
                'ì£¼ìš” í‚¤ì›Œë“œ': keywords
            })
        keywords_df = pd.DataFrame(keywords_data)
        
        with pd.ExcelWriter(output_excel, engine='openpyxl') as writer:
            result_df.to_excel(writer, index=False, sheet_name='ì„ íƒí•œí† í”½' if selected_topics else 'ì „ì²´í† í”½')
            keywords_df.to_excel(writer, index=False, sheet_name='í† í”½í‚¤ì›Œë“œ')
        
        # ëª¨ë¸ ì €ì¥
        model_path = f"{OUTPUT_DIR}/lda_model_{n_topics}_topics.model"
        model.save(model_path)
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        metadata = {
            'n_topics': n_topics,
            'total_documents': len(result_df),
            'selected_topics': selected_topics if selected_topics else list(range(n_topics)),
            'coherence_score': float(self.coherence_scores[n_topics]),
            'perplexity_score': float(self.perplexity_scores[n_topics]),
            'parameters': {
                'passes': PASSES,
                'iterations': ITERATIONS,
                'alpha': str(ALPHA),
                'eta': str(ETA),
                'no_below': NO_BELOW,
                'no_above': NO_ABOVE,
                'keep_n': KEEP_N
            },
            'timestamp': timestamp
        }
        
        meta_path = f"{OUTPUT_DIR}/lda_{n_topics}_topics{suffix}_metadata_{timestamp}.json"
        with open(meta_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        if self.verbose:
            print(f"âœ… ì €ì¥ ì™„ë£Œ!")
            print(f"   - CSV: {output_csv}")
            print(f"   - Excel: {output_excel}")
            print(f"   - ëª¨ë¸: {model_path}")
            print(f"   - ë©”íƒ€ë°ì´í„°: {meta_path}")
        
        return result_df
    
    def print_topics(self, n_topics, top_n=10):
        """í† í”½ë³„ í‚¤ì›Œë“œ ì¶œë ¥"""
        print(f"\n{'='*80}")
        print(f"ğŸ“‹ í† í”½ë³„ ì£¼ìš” í‚¤ì›Œë“œ ({n_topics}ê°œ í† í”½, Top {top_n})")
        print(f"{'='*80}")
        
        model = self.models[n_topics]
        topics = self.topics_dict[n_topics]
        
        for topic_id in range(n_topics):
            count = (topics == topic_id).sum()
            words = model.show_topic(topic_id, topn=top_n)
            keywords = ', '.join([f"{word}({prob:.3f})" for word, prob in words])
            
            print(f"\n[Topic {topic_id}] ({count:,}ê°œ ë¬¸ì„œ)")
            print(f"  {keywords}")
        
        print(f"\n{'='*80}")
    
    def print_summary(self):
        """ì „ì²´ ê²°ê³¼ ìš”ì•½"""
        print(f"\n{'='*80}")
        print("ğŸ“Š ì „ì²´ í•™ìŠµ ê²°ê³¼ ìš”ì•½")
        print(f"{'='*80}")
        
        # ìš”ì•½ í…Œì´ë¸”
        summary_data = []
        for n_topics in sorted(self.models.keys()):
            topics = self.topics_dict[n_topics]
            coherence = self.coherence_scores[n_topics]
            perplexity = self.perplexity_scores[n_topics]
            doc_count = (topics != -1).sum()
            
            summary_data.append({
                'í† í”½ ê°œìˆ˜': n_topics,
                'ë¬¸ì„œ ìˆ˜': f"{doc_count:,}",
                'Coherence': f"{coherence:.4f}",
                'Perplexity': f"{perplexity:.2f}"
            })
        
        summary_df = pd.DataFrame(summary_data)
        print("\n" + summary_df.to_string(index=False))
        print(f"\n{'='*80}")

# ============================================================================
# ì—˜ë³´ìš° í¬ì¸íŠ¸ ê³„ì‚°
# ============================================================================
def calculate_elbow_point(scores_dict, maximize=True):
    """ì—˜ë³´ìš° í¬ì¸íŠ¸ ê³„ì‚°"""
    if len(scores_dict) < 3:
        return None
    
    topics = np.array(sorted(scores_dict.keys()))
    scores = np.array([scores_dict[k] for k in topics])
    
    if not maximize:
        scores = -scores
    
    scores_norm = (scores - scores.min()) / (scores.max() - scores.min() + 1e-10)
    topics_norm = (topics - topics.min()) / (topics.max() - topics.min() + 1e-10)
    
    m = (scores_norm[-1] - scores_norm[0]) / (topics_norm[-1] - topics_norm[0] + 1e-10)
    b = scores_norm[0]
    
    distances = np.abs(scores_norm - (m * topics_norm + b)) / np.sqrt(m**2 + 1)
    elbow_idx = np.argmax(distances)
    elbow_point = topics[elbow_idx]
    
    return int(elbow_point)

# ============================================================================
# ë©”ì¸ í•¨ìˆ˜
# ============================================================================
def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    start_time = time.time()
    
    print("="*80)
    print("ğŸ¯ LDA í† í”½ ëª¨ë¸ë§ ì‹œì‘")
    print("="*80)
    
    # ========================================
    # 1. ë°ì´í„° ë¡œë“œ
    # ========================================
    print("\nğŸ“ 1. ë°ì´í„° ë¡œë“œ")
    print(f"   ì…ë ¥ íŒŒì¼: {INPUT_CSV}")
    
    if not os.path.exists(INPUT_CSV):
        print(f"\nâŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {INPUT_CSV}")
        print("íŒíŠ¸: INPUT_CSV ê²½ë¡œë¥¼ ìˆ˜ì •í•˜ì„¸ìš”.")
        return
    
    try:
        df = pd.read_csv(INPUT_CSV)
        print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df):,}ê°œ ë¬¸ì„œ")
        print(f"   ì»¬ëŸ¼: {list(df.columns)}")
        
        if 'sentence' not in df.columns:
            print("\nâŒ 'sentence' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
            
    except Exception as e:
        print(f"\nâŒ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return
    
    # ========================================
    # 2. íŒŒë¼ë¯¸í„° í™•ì¸
    # ========================================
    print("\nâš™ï¸ 2. íŒŒë¼ë¯¸í„° ì„¤ì •")
    print(f"   í† í”½ ê°œìˆ˜: {TOPIC_NUMBERS}")
    print(f"   Passes: {PASSES}, Iterations: {ITERATIONS}")
    print(f"   Alpha: {ALPHA}, Eta: {ETA}")
    print(f"   Dictionary í•„í„°ë§: no_below={NO_BELOW}, no_above={NO_ABOVE}, keep_n={KEEP_N}")
    print(f"   ìµœì†Œ ëª…ì‚¬ ê¸¸ì´: {MIN_NOUN_LENGTH}")
    print(f"   ë¶ˆìš©ì–´: {len(STOP_WORDS)}ê°œ")
    
    # ========================================
    # 3. LDA ì‹¤í–‰
    # ========================================
    print("\nğŸš€ 3. LDA í† í”½ ëª¨ë¸ë§ ì‹¤í–‰")
    
    lda = LDATopicModeling(df, STOP_WORDS, MIN_NOUN_LENGTH, verbose=True)
    
    # ì „ì²˜ë¦¬
    lda.preprocess(use_cache=True)
    
    # Dictionary & Corpus ìƒì„±
    lda.create_dict_corpus(NO_BELOW, NO_ABOVE, KEEP_N)
    
    # LDA í•™ìŠµ (ì—¬ëŸ¬ í† í”½ ê°œìˆ˜)
    print(f"\n{'='*80}")
    print("ğŸ“š LDA ëª¨ë¸ í•™ìŠµ")
    print(f"{'='*80}")
    
    for i, n_topics in enumerate(TOPIC_NUMBERS, 1):
        print(f"\n[{i}/{len(TOPIC_NUMBERS)}] {n_topics}ê°œ í† í”½ í•™ìŠµ")
        lda.train_lda(n_topics, PASSES, ITERATIONS, ALPHA, ETA)
    
    # ========================================
    # 4. ê²°ê³¼ ìš”ì•½
    # ========================================
    lda.print_summary()
    
    # ì—˜ë³´ìš° í¬ì¸íŠ¸ ê³„ì‚°
    coherence_elbow = calculate_elbow_point(lda.coherence_scores, maximize=True)
    perplexity_elbow = calculate_elbow_point(lda.perplexity_scores, maximize=False)
    
    print("\nğŸ¯ ì¶”ì²œ í† í”½ ê°œìˆ˜ (ì—˜ë³´ìš° í¬ì¸íŠ¸)")
    if coherence_elbow and perplexity_elbow:
        if coherence_elbow == perplexity_elbow:
            print(f"   â­ {coherence_elbow}ê°œ í† í”½ (Coherenceì™€ Perplexity ëª¨ë‘ ìµœì )")
        else:
            print(f"   - Coherence ê¸°ì¤€: {coherence_elbow}ê°œ í† í”½")
            print(f"   - Perplexity ê¸°ì¤€: {perplexity_elbow}ê°œ í† í”½")
    
    # ========================================
    # 5. ê²°ê³¼ ì €ì¥
    # ========================================
    print(f"\n{'='*80}")
    print("ğŸ’¾ 4. ê²°ê³¼ ì €ì¥")
    print(f"{'='*80}")
    
    # ê°€ì¥ ì¢‹ì€ í† í”½ ê°œìˆ˜ ì„ íƒ (Coherence ê¸°ì¤€)
    best_n_topics = max(lda.coherence_scores.keys(),
                        key=lambda k: lda.coherence_scores[k])
    
    print(f"\nğŸ“Œ ì €ì¥í•  í† í”½ ê°œìˆ˜: {best_n_topics}ê°œ (Coherence ìµœê³ )")
    
    # í† í”½ë³„ í‚¤ì›Œë“œ ì¶œë ¥
    lda.print_topics(best_n_topics, top_n=10)
    
    # í† í”½ ì„ íƒ UI
    print(f"\n{'='*80}")
    print("ğŸ¯ ì €ì¥í•  í† í”½ ì„ íƒ")
    print(f"{'='*80}")
    
    result_df = lda.get_result_df(best_n_topics)
    model = lda.models[best_n_topics]
    
    # í† í”½ë³„ ì •ë³´ ì¶œë ¥
    print(f"\ní† í”½ë³„ ë¬¸ì„œ ìˆ˜:")
    for topic_id in range(best_n_topics):
        count = (result_df['lda_topic'] == topic_id).sum()
        pct = count / len(result_df) * 100
        words = model.show_topic(topic_id, topn=3)
        keywords = ', '.join([word for word, _ in words])
        print(f"  Topic {topic_id}: {count:,}ê°œ ({pct:.1f}%) - {keywords}")
    
    # ì‚¬ìš©ì ì…ë ¥
    print(f"\nğŸ’¡ ì €ì¥í•  í† í”½ì„ ì„ íƒí•˜ì„¸ìš”:")
    print("   1. ì „ì²´ í† í”½ ì €ì¥ (Enter ë˜ëŠ” 'all' ì…ë ¥)")
    print("   2. íŠ¹ì • í† í”½ë§Œ ì €ì¥ (ì˜ˆ: 0,2,5 ë˜ëŠ” 0-5 ë˜ëŠ” 0-5,9,11)")
    
    user_input = input("\nì„ íƒ: ").strip()
    
    selected_topics = None
    
    if user_input == '' or user_input.lower() == 'all':
        # ì „ì²´ ì €ì¥
        print(f"âœ… ì „ì²´ {best_n_topics}ê°œ í† í”½ ì €ì¥")
        selected_topics = None
    else:
        # íŠ¹ì • í† í”½ íŒŒì‹±
        try:
            selected_topics = []
            
            # ì‰¼í‘œë¡œ ë¶„ë¦¬
            parts = user_input.split(',')
            
            for part in parts:
                part = part.strip()
                
                # ë²”ìœ„ ì…ë ¥ ì²˜ë¦¬ (ì˜ˆ: 0-5)
                if '-' in part:
                    start, end = map(int, part.split('-'))
                    selected_topics.extend(range(start, end + 1))
                else:
                    # ë‹¨ì¼ ìˆ«ì
                    selected_topics.append(int(part))
            
            # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
            selected_topics = sorted(list(set(selected_topics)))
            
            # ìœ íš¨ì„± ê²€ì‚¬
            selected_topics = [t for t in selected_topics if 0 <= t < best_n_topics]
            
            if not selected_topics:
                print("âš ï¸ ìœ íš¨í•œ í† í”½ì´ ì—†ìŠµë‹ˆë‹¤. ì „ì²´ í† í”½ì„ ì €ì¥í•©ë‹ˆë‹¤.")
                selected_topics = None
            else:
                print(f"âœ… {len(selected_topics)}ê°œ í† í”½ ì„ íƒ: {selected_topics}")
                
                # ì„ íƒí•œ í† í”½ ì •ë³´ ì¶œë ¥
                selected_count = result_df[result_df['lda_topic'].isin(selected_topics)].shape[0]
                selected_pct = selected_count / len(result_df) * 100
                print(f"   - ì„ íƒí•œ í† í”½ì˜ ë¬¸ì„œ ìˆ˜: {selected_count:,}ê°œ ({selected_pct:.1f}%)")
                
        except Exception as e:
            print(f"âš ï¸ ì…ë ¥ í˜•ì‹ ì˜¤ë¥˜: {e}")
            print("   ì „ì²´ í† í”½ì„ ì €ì¥í•©ë‹ˆë‹¤.")
            selected_topics = None
    
    # ê²°ê³¼ ì €ì¥
    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘...")
    lda.save_results(best_n_topics, selected_topics=selected_topics)
    
    # Dictionary ì €ì¥
    dict_path = f"{OUTPUT_DIR}/lda_dictionary.dict"
    lda.dictionary.save(dict_path)
    print(f"âœ… Dictionary ì €ì¥: {dict_path}")
    
    # ========================================
    # 6. ì™„ë£Œ
    # ========================================
    total_time = time.time() - start_time
    
    print(f"\n{'='*80}")
    print("âœ… LDA í† í”½ ëª¨ë¸ë§ ì™„ë£Œ!")
    print(f"{'='*80}")
    print(f"ì´ ì‹¤í–‰ ì‹œê°„: {total_time/60:.1f}ë¶„")
    print(f"ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {OUTPUT_DIR}/")
    print(f"{'='*80}\n")

# ============================================================================
# ì‹¤í–‰
# ============================================================================
if __name__ == "__main__":
    main()
