# -*- coding: utf-8 -*-
"""
BERTopic í† í”½ ëª¨ë¸ë§ ìë™í™” (ë¡œì»¬ ë²„ì „)
"""

import time
import os
import pickle
import hashlib
import json
from pathlib import Path

import pandas as pd
import numpy as np
import re
from tqdm import tqdm

# BERTopic & Related
from bertopic import BERTopic
from umap import UMAP
from hdbscan import HDBSCAN
from sklearn.feature_extraction.text import CountVectorizer
from sentence_transformers import SentenceTransformer

# ============================================================================
# ì„¤ì •
# ============================================================================
INPUT_CSV = '/Users/song/Desktop/workspace/fin/hv_labeled.csv'  # ì…ë ¥ CSV íŒŒì¼ ê²½ë¡œ
OUTPUT_DIR = './BERTopic_results'         # ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
CACHE_DIR = './BERTopic_cache'            # ìºì‹œ ë””ë ‰í† ë¦¬

# ì„ë² ë”© ëª¨ë¸
EMBEDDING_MODEL = 'jhgan/ko-sroberta-multitask'  # 'jhgan/ko-sroberta-multitask', 'paraphrase-multilingual-MiniLM-L12-v2'

# UMAP íŒŒë¼ë¯¸í„°
N_COMPONENTS = 5
N_NEIGHBORS = 15
MIN_DIST = 0.0

# HDBSCAN íŒŒë¼ë¯¸í„°
MIN_CLUSTER_SIZE = 50
MIN_SAMPLES = 10

# í† í”½ ê°œìˆ˜ ì„¤ì •
TOPIC_MODE = 'auto'  # 'auto' ë˜ëŠ” ìˆ«ì (ì˜ˆ: 20)

# Vectorizer íŒŒë¼ë¯¸í„°
MAX_FEATURES = 200
MAX_DF = 0.8
NGRAM_MAX = 1

# ìƒ˜í”Œë§ ì„¤ì •
USE_SAMPLING = False  # ëŒ€ìš©ëŸ‰ ë°ì´í„°ì¼ ê²½ìš° True
SAMPLE_SIZE = 50000   # ìƒ˜í”Œë§ í¬ê¸°

# ë””ë ‰í† ë¦¬ ìƒì„±
os.makedirs(CACHE_DIR, exist_ok=True)
os.makedirs(OUTPUT_DIR, exist_ok=True)

# ============================================================================
# í—¬í¼ í•¨ìˆ˜
# ============================================================================
def smart_tokenizer(text):
    """ìŠ¤ë§ˆíŠ¸ í† í¬ë‚˜ì´ì €"""
    pattern = r'\b[ê°€-í£]{2,}\b|\b[A-Z]{2,}\b|\b[a-z]{3,}\b'
    tokens = re.findall(pattern, text.lower())
    filtered = []
    for token in tokens:
        if any(char.isdigit() for char in token):
            continue
        if len(token) < 2:
            continue
        filtered.append(token)
    return filtered

# ============================================================================
# BERTopic í´ë˜ìŠ¤
# ============================================================================
class BERTopicModeling:
    """BERTopic í† í”½ ëª¨ë¸ë§ ìë™í™”"""
    
    def __init__(self, df, verbose=True):
        self.df = df
        self.verbose = verbose
        self.embeddings = None
        self.topic_model = None
        self.topics = None
        self.df_result = None
    
    def create_embeddings(self, use_cache=True):
        """ì„ë² ë”© ìƒì„± (ìºì‹œ í™œìš©)"""
        texts = self.df['sentence'].tolist()
        
        # ìºì‹œ íŒŒì¼ëª… ìƒì„±
        data_hash = hashlib.md5(
            (self.df['sentence'].str.cat() + EMBEDDING_MODEL).encode()
        ).hexdigest()[:8]
        cache_file = f"{CACHE_DIR}/embeddings_{data_hash}.pkl"
        
        # ìºì‹œ ë¡œë“œ
        if use_cache and os.path.exists(cache_file):
            if self.verbose:
                print("ğŸ“¦ ìºì‹œëœ ì„ë² ë”© ë¡œë“œ ì¤‘...")
            with open(cache_file, 'rb') as f:
                self.embeddings = pickle.load(f)
            if self.verbose:
                print(f"âœ… ì„ë² ë”© ë¡œë“œ ì™„ë£Œ (ìºì‹œ): {self.embeddings.shape}")
            return texts
        
        # ì„ë² ë”© ìƒì„±
        if self.verbose:
            print("\nğŸ”¤ ì„ë² ë”© ìƒì„± ì‹œì‘...")
            print(f"   ëª¨ë¸: {EMBEDDING_MODEL}")
        
        model = SentenceTransformer(EMBEDDING_MODEL)
        
        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì„ë² ë”© ìƒì„±
        batch_size = 32
        embeddings_list = []
        
        for i in tqdm(range(0, len(texts), batch_size), desc="ì„ë² ë”© ìƒì„±", disable=not self.verbose):
            batch = texts[i:i+batch_size]
            batch_embeddings = model.encode(
                batch,
                batch_size=batch_size,
                show_progress_bar=False,
                convert_to_numpy=True
            )
            embeddings_list.append(batch_embeddings)
        
        self.embeddings = np.vstack(embeddings_list)
        
        # ìºì‹œ ì €ì¥
        with open(cache_file, 'wb') as f:
            pickle.dump(self.embeddings, f)
        
        if self.verbose:
            print(f"âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ: {self.embeddings.shape}")
        
        return texts
    
    def train_bertopic(self, texts, sample_texts=None, sample_embeddings=None):
        """BERTopic í•™ìŠµ"""
        if self.verbose:
            print("\nğŸš€ BERTopic ëª¨ë¸ í•™ìŠµ")
            print(f"   - UMAP: n_components={N_COMPONENTS}, n_neighbors={N_NEIGHBORS}, min_dist={MIN_DIST}")
            print(f"   - HDBSCAN: min_cluster_size={MIN_CLUSTER_SIZE}, min_samples={MIN_SAMPLES}")
            print(f"   - í† í”½ ê°œìˆ˜: {TOPIC_MODE}")
        
        start_time = time.time()
        
        # UMAP
        umap_model = UMAP(
            n_components=N_COMPONENTS,
            n_neighbors=N_NEIGHBORS,
            min_dist=MIN_DIST,
            metric='cosine',
            random_state=42
        )
        
        # HDBSCAN
        hdbscan_model = HDBSCAN(
            min_cluster_size=MIN_CLUSTER_SIZE,
            min_samples=MIN_SAMPLES,
            cluster_selection_method='eom',
            metric='euclidean',
            prediction_data=False
        )
        
        # Vectorizer
        vectorizer_model = CountVectorizer(
            tokenizer=smart_tokenizer,
            max_features=MAX_FEATURES,
            max_df=MAX_DF,
            ngram_range=(1, NGRAM_MAX)
        )
        
        # BERTopic
        self.topic_model = BERTopic(
            umap_model=umap_model,
            hdbscan_model=hdbscan_model,
            vectorizer_model=vectorizer_model,
            nr_topics=TOPIC_MODE if TOPIC_MODE == 'auto' else int(TOPIC_MODE),
            min_topic_size=max(10, int(len(texts) * 0.001)),
            calculate_probabilities=False,
            verbose=False
        )
        
        # í•™ìŠµ
        if sample_texts is not None:
            if self.verbose:
                print(f"   ğŸ“Š ìƒ˜í”Œë¡œ í•™ìŠµ ì¤‘... ({len(sample_texts):,}ê°œ)")
            self.topics, _ = self.topic_model.fit_transform(sample_texts, sample_embeddings)
        else:
            if self.verbose:
                print(f"   ğŸ“Š ì „ì²´ ë°ì´í„°ë¡œ í•™ìŠµ ì¤‘... ({len(texts):,}ê°œ)")
            self.topics, _ = self.topic_model.fit_transform(texts, self.embeddings)
        
        self.topics = np.array(self.topics)
        
        elapsed = time.time() - start_time
        
        if self.verbose:
            print(f"âœ… í•™ìŠµ ì™„ë£Œ! (ì†Œìš” ì‹œê°„: {elapsed:.1f}ì´ˆ)")
    
    def predict_all(self, texts):
        """ì „ì²´ ë°ì´í„° ì˜ˆì¸¡ (ìƒ˜í”Œë§ ì‚¬ìš© ì‹œ)"""
        if self.verbose:
            print("\nğŸ“Š ì „ì²´ ë°ì´í„° ì˜ˆì¸¡ ì¤‘...")
        
        self.topics, _ = self.topic_model.transform(texts, self.embeddings)
        self.topics = np.array(self.topics)
        
        if self.verbose:
            print("âœ… ì˜ˆì¸¡ ì™„ë£Œ!")
    
    def create_result_df(self):
        """ê²°ê³¼ ë°ì´í„°í”„ë ˆì„ ìƒì„±"""
        self.df_result = self.df.copy()
        self.df_result['bertopic_topic'] = self.topics
        self.df_result['outlier'] = (self.topics == -1).astype(int)
        
        # í†µê³„
        outlier_count = (self.topics == -1).sum()
        outlier_pct = outlier_count / len(self.topics) * 100
        unique_topics = sorted([t for t in set(self.topics) if t != -1])
        n_topics = len(unique_topics)
        
        if self.verbose:
            print(f"\n{'='*80}")
            print("ğŸ“Š í•™ìŠµ ê²°ê³¼")
            print(f"{'='*80}")
            print(f"   - ì „ì²´ ë¬¸ì„œ: {len(self.topics):,}ê°œ")
            print(f"   - í† í”½ ìˆ˜: {n_topics}ê°œ")
            print(f"   - Outlier: {outlier_count:,}ê°œ ({outlier_pct:.1f}%)")
            
            if outlier_pct < 25:
                status = "ìš°ìˆ˜"
            elif outlier_pct < 35:
                status = "ì–‘í˜¸"
            elif outlier_pct < 45:
                status = "ë³´í†µ"
            else:
                status = "ê°œì„ í•„ìš”"
            print(f"   - í‰ê°€: {status}")
            print(f"{'='*80}")
        
        return unique_topics
    
    def print_topics(self, unique_topics, top_n=10):
        """í† í”½ë³„ í‚¤ì›Œë“œ ì¶œë ¥"""
        print(f"\n{'='*80}")
        print(f"ğŸ“‹ í† í”½ë³„ ì£¼ìš” í‚¤ì›Œë“œ (Top {top_n})")
        print(f"{'='*80}")
        
        for topic_id in unique_topics[:20]:  # ìƒìœ„ 20ê°œë§Œ
            count = (self.topics == topic_id).sum()
            pct = count / len(self.topics) * 100
            words = self.topic_model.get_topic(topic_id)
            
            if words:
                keywords = ', '.join([f"{w[0]}({w[1]:.3f})" for w in words[:top_n]])
                print(f"\n[Topic {topic_id}] ({count:,}ê°œ ë¬¸ì„œ, {pct:.1f}%)")
                print(f"  {keywords}")
        
        print(f"\n{'='*80}")
    
    def save_results(self, unique_topics, selected_topics=None):
        """ê²°ê³¼ ì €ì¥"""
        if self.verbose:
            print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘...")
        
        # ì„ íƒí•œ í† í”½ë§Œ í•„í„°ë§
        if selected_topics is not None:
            result_df = self.df_result[self.df_result['bertopic_topic'].isin(selected_topics)].copy()
            suffix = f"_selected_{len(selected_topics)}topics"
        else:
            result_df = self.df_result.copy()
            suffix = ""
        
        timestamp = pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')
        
        # CSV ì €ì¥
        output_csv = f"{OUTPUT_DIR}/bertopic_result{suffix}_{timestamp}.csv"
        result_df.to_csv(output_csv, index=False, encoding='utf-8-sig')
        
        # Excel ì €ì¥ (í‚¤ì›Œë“œ í¬í•¨)
        output_excel = f"{OUTPUT_DIR}/bertopic_result{suffix}_{timestamp}.xlsx"
        
        # í† í”½ë³„ í‚¤ì›Œë“œ í…Œì´ë¸” ìƒì„±
        keywords_data = []
        for topic_id in unique_topics:
            count = (self.topics == topic_id).sum()
            pct = count / len(self.topics) * 100
            words = self.topic_model.get_topic(topic_id)
            
            if words:
                keywords = ', '.join([f"{w[0]}({w[1]:.3f})" for w in words[:5]])
                keywords_data.append({
                    'í† í”½': f"Topic {topic_id}",
                    'ë¬¸ì„œ ìˆ˜': f"{count:,}",
                    'ë¹„ìœ¨': f"{pct:.1f}%",
                    'ì£¼ìš” í‚¤ì›Œë“œ': keywords
                })
        
        keywords_df = pd.DataFrame(keywords_data)
        
        with pd.ExcelWriter(output_excel, engine='openpyxl') as writer:
            result_df.to_excel(writer, index=False, sheet_name='ì„ íƒí•œí† í”½' if selected_topics else 'ì „ì²´í† í”½')
            keywords_df.to_excel(writer, index=False, sheet_name='í† í”½í‚¤ì›Œë“œ')
        
        # ëª¨ë¸ ì €ì¥
        model_path = f"{OUTPUT_DIR}/bertopic_model.pkl"
        self.topic_model.save(model_path, serialization='pickle')
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        outlier_count = (self.topics == -1).sum()
        outlier_pct = outlier_count / len(self.topics) * 100
        
        metadata = {
            'n_topics': int(len(unique_topics)),
            'total_documents': int(len(result_df)),
            'selected_topics': [int(t) for t in (selected_topics if selected_topics else unique_topics)],
            'outlier_count': int(outlier_count),
            'outlier_percentage': float(outlier_pct),
            'parameters': {
                'embedding_model': EMBEDDING_MODEL,
                'n_components': int(N_COMPONENTS),
                'n_neighbors': int(N_NEIGHBORS),
                'min_dist': float(MIN_DIST),
                'min_cluster_size': int(MIN_CLUSTER_SIZE),
                'min_samples': int(MIN_SAMPLES),
                'topic_mode': str(TOPIC_MODE),
                'max_features': int(MAX_FEATURES),
                'max_df': float(MAX_DF),
                'ngram_range': f"(1, {NGRAM_MAX})"
            },
            'timestamp': timestamp
        }
        
        meta_path = f"{OUTPUT_DIR}/bertopic_metadata{suffix}_{timestamp}.json"
        with open(meta_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        if self.verbose:
            print(f"âœ… ì €ì¥ ì™„ë£Œ!")
            print(f"   - CSV: {output_csv}")
            print(f"   - Excel: {output_excel}")
            print(f"   - ëª¨ë¸: {model_path}")
            print(f"   - ë©”íƒ€ë°ì´í„°: {meta_path}")

# ============================================================================
# ë©”ì¸ í•¨ìˆ˜
# ============================================================================
def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    start_time = time.time()
    
    print("="*80)
    print("ğŸ¯ BERTopic í† í”½ ëª¨ë¸ë§ ì‹œì‘")
    print("="*80)
    
    # ========================================
    # 1. ë°ì´í„° ë¡œë“œ
    # ========================================
    print("\nğŸ“ 1. ë°ì´í„° ë¡œë“œ")
    print(f"   ì…ë ¥ íŒŒì¼: {INPUT_CSV}")
    
    if not os.path.exists(INPUT_CSV):
        print(f"\nâŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {INPUT_CSV}")
        print("íŒíŠ¸: INPUT_CSV ê²½ë¡œë¥¼ ìˆ˜ì •í•˜ì„¸ìš”.")
        return
    
    try:
        df = pd.read_csv(INPUT_CSV)
        print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df):,}ê°œ ë¬¸ì„œ")
        print(f"   ì»¬ëŸ¼: {list(df.columns)}")
        
        if 'sentence' not in df.columns:
            print("\nâŒ 'sentence' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
            
    except Exception as e:
        print(f"\nâŒ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return
    
    # ========================================
    # 2. íŒŒë¼ë¯¸í„° í™•ì¸
    # ========================================
    print("\nâš™ï¸ 2. íŒŒë¼ë¯¸í„° ì„¤ì •")
    print(f"   ì„ë² ë”© ëª¨ë¸: {EMBEDDING_MODEL}")
    print(f"   UMAP: n_components={N_COMPONENTS}, n_neighbors={N_NEIGHBORS}, min_dist={MIN_DIST}")
    print(f"   HDBSCAN: min_cluster_size={MIN_CLUSTER_SIZE}, min_samples={MIN_SAMPLES}")
    print(f"   í† í”½ ê°œìˆ˜: {TOPIC_MODE}")
    print(f"   Vectorizer: max_features={MAX_FEATURES}, max_df={MAX_DF}, ngram=(1,{NGRAM_MAX})")
    print(f"   ìƒ˜í”Œë§: {'ì‚¬ìš©' if USE_SAMPLING else 'ë¯¸ì‚¬ìš©'}" + (f" ({SAMPLE_SIZE:,}ê°œ)" if USE_SAMPLING else ""))
    
    # ========================================
    # 3. BERTopic ì‹¤í–‰
    # ========================================
    print("\nğŸš€ 3. BERTopic í† í”½ ëª¨ë¸ë§ ì‹¤í–‰")
    
    bertopic = BERTopicModeling(df, verbose=True)
    
    # ì„ë² ë”© ìƒì„±
    texts = bertopic.create_embeddings(use_cache=True)
    
    # ìƒ˜í”Œë§ (ì˜µì…˜)
    if USE_SAMPLING and SAMPLE_SIZE < len(texts):
        print(f"\nğŸ“Š ìƒ˜í”Œë§ ì‚¬ìš©: {SAMPLE_SIZE:,}ê°œë¡œ í•™ìŠµ")
        np.random.seed(42)
        sample_indices = np.random.choice(len(bertopic.embeddings), SAMPLE_SIZE, replace=False)
        sample_embeddings = bertopic.embeddings[sample_indices]
        sample_texts = [texts[i] for i in sample_indices]
        
        # í•™ìŠµ
        bertopic.train_bertopic(texts, sample_texts, sample_embeddings)
        
        # ì „ì²´ ë°ì´í„° ì˜ˆì¸¡
        bertopic.predict_all(texts)
    else:
        print(f"\nğŸ“Š ì „ì²´ ë°ì´í„° ì‚¬ìš©: {len(texts):,}ê°œ")
        bertopic.train_bertopic(texts)
    
    # ========================================
    # 4. ê²°ê³¼ ìƒì„±
    # ========================================
    unique_topics = bertopic.create_result_df()
    
    # í† í”½ë³„ í‚¤ì›Œë“œ ì¶œë ¥
    bertopic.print_topics(unique_topics, top_n=10)
    
    # ========================================
    # 5. í† í”½ ì„ íƒ ë° ì €ì¥
    # ========================================
    print(f"\n{'='*80}")
    print("ğŸ¯ ì €ì¥í•  í† í”½ ì„ íƒ")
    print(f"{'='*80}")
    
    # Outlier í¬í•¨ ì—¬ë¶€
    print(f"\nğŸ’¡ Outlier (-1) í† í”½ì„ í¬í•¨í•˜ì‹œê² ìŠµë‹ˆê¹Œ?")
    include_outlier = input("   í¬í•¨ (y/n) [ê¸°ë³¸: n]: ").strip().lower()
    
    available_topics = unique_topics.copy()
    if include_outlier == 'y':
        available_topics = [-1] + available_topics
        print("âœ… Outlier í¬í•¨")
    else:
        print("âœ… Outlier ì œì™¸")
    
    # í† í”½ë³„ ì •ë³´ ì¶œë ¥
    print(f"\ní† í”½ë³„ ë¬¸ì„œ ìˆ˜:")
    for topic_id in available_topics[:30]:  # ìµœëŒ€ 30ê°œë§Œ í‘œì‹œ
        if topic_id == -1:
            count = (bertopic.topics == -1).sum()
            pct = count / len(bertopic.topics) * 100
            print(f"  Topic {topic_id} (Outlier): {count:,}ê°œ ({pct:.1f}%)")
        else:
            count = (bertopic.topics == topic_id).sum()
            pct = count / len(bertopic.topics) * 100
            words = bertopic.topic_model.get_topic(topic_id)
            if words:
                keywords = ', '.join([w[0] for w in words[:3]])
                print(f"  Topic {topic_id}: {count:,}ê°œ ({pct:.1f}%) - {keywords}")
    
    if len(available_topics) > 30:
        print(f"  ... (ì´ {len(available_topics)}ê°œ í† í”½)")
    
    # ì‚¬ìš©ì ì…ë ¥
    print(f"\nğŸ’¡ ì €ì¥í•  í† í”½ì„ ì„ íƒí•˜ì„¸ìš”:")
    print("   1. ì „ì²´ í† í”½ ì €ì¥ (Enter ë˜ëŠ” 'all' ì…ë ¥)")
    print("   2. íŠ¹ì • í† í”½ë§Œ ì €ì¥ (ì˜ˆ: 0,2,5 ë˜ëŠ” 0-5 ë˜ëŠ” 0-5,9,11)")
    
    user_input = input("\nì„ íƒ: ").strip()
    
    selected_topics = None
    
    if user_input == '' or user_input.lower() == 'all':
        # ì „ì²´ ì €ì¥
        print(f"âœ… ì „ì²´ {len(available_topics)}ê°œ í† í”½ ì €ì¥")
        selected_topics = None
    else:
        # íŠ¹ì • í† í”½ íŒŒì‹±
        try:
            selected_topics = []
            
            # ì‰¼í‘œë¡œ ë¶„ë¦¬
            parts = user_input.split(',')
            
            for part in parts:
                part = part.strip()
                
                # ë²”ìœ„ ì…ë ¥ ì²˜ë¦¬ (ì˜ˆ: 0-5)
                if '-' in part:
                    start, end = map(int, part.split('-'))
                    selected_topics.extend(range(start, end + 1))
                else:
                    # ë‹¨ì¼ ìˆ«ì
                    selected_topics.append(int(part))
            
            # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
            selected_topics = sorted(list(set(selected_topics)))
            
            # ìœ íš¨ì„± ê²€ì‚¬
            selected_topics = [t for t in selected_topics if t in available_topics]
            
            if not selected_topics:
                print("âš ï¸ ìœ íš¨í•œ í† í”½ì´ ì—†ìŠµë‹ˆë‹¤. ì „ì²´ í† í”½ì„ ì €ì¥í•©ë‹ˆë‹¤.")
                selected_topics = None
            else:
                print(f"âœ… {len(selected_topics)}ê°œ í† í”½ ì„ íƒ: {selected_topics}")
                
                # ì„ íƒí•œ í† í”½ ì •ë³´ ì¶œë ¥
                selected_count = bertopic.df_result[bertopic.df_result['bertopic_topic'].isin(selected_topics)].shape[0]
                selected_pct = selected_count / len(bertopic.df_result) * 100
                print(f"   - ì„ íƒí•œ í† í”½ì˜ ë¬¸ì„œ ìˆ˜: {selected_count:,}ê°œ ({selected_pct:.1f}%)")
                
        except Exception as e:
            print(f"âš ï¸ ì…ë ¥ í˜•ì‹ ì˜¤ë¥˜: {e}")
            print("   ì „ì²´ í† í”½ì„ ì €ì¥í•©ë‹ˆë‹¤.")
            selected_topics = None
    
    # ê²°ê³¼ ì €ì¥
    bertopic.save_results(unique_topics, selected_topics=selected_topics)
    
    # ========================================
    # 6. ì™„ë£Œ
    # ========================================
    total_time = time.time() - start_time
    
    print(f"\n{'='*80}")
    print("âœ… BERTopic í† í”½ ëª¨ë¸ë§ ì™„ë£Œ!")
    print(f"{'='*80}")
    print(f"ì´ ì‹¤í–‰ ì‹œê°„: {total_time/60:.1f}ë¶„")
    print(f"ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {OUTPUT_DIR}/")
    print(f"{'='*80}\n")

# ============================================================================
# ì‹¤í–‰
# ============================================================================
if __name__ == "__main__":
    main()
