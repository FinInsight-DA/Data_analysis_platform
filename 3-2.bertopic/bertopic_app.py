# -*- coding: utf-8 -*-
"""
BERTopic í† í”½ ëª¨ë¸ë§ Streamlit ì•± (ë¶„ì„ê°€ìš©)
"""

import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
import time
import re
import os
from pathlib import Path
from io import BytesIO
from datetime import datetime
import json

# BERTopic & Related
from bertopic import BERTopic
from umap import UMAP
from hdbscan import HDBSCAN
from sklearn.feature_extraction.text import CountVectorizer
from sentence_transformers import SentenceTransformer

# ============================================================================
# í˜ì´ì§€ ì„¤ì •
# ============================================================================
st.set_page_config(
    page_title="BERTopic í† í”½ ëª¨ë¸ë§",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ============================================================================
# CSS ìŠ¤íƒ€ì¼
# ============================================================================
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
    }
    .sub-header {
        font-size: 1.5rem;
        font-weight: bold;
        color: #2c3e50;
        margin-top: 2rem;
        margin-bottom: 1rem;
    }
</style>
""", unsafe_allow_html=True)

# ============================================================================
# í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸ ëª©ë¡
# ============================================================================
EMBEDDING_MODELS = {
    'jhgan/ko-sroberta-multitask': 'KoSRoBERTa (ì¶”ì²œ)',
    'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2': 'Multilingual MiniLM',
    'sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens': 'XLM-R 100langs'
}

# ============================================================================
# í—¬í¼ í•¨ìˆ˜
# ============================================================================
def scale_parameters(data_size, base_size=309513):
    """ë°ì´í„° í¬ê¸°ì— ë”°ë¥¸ íŒŒë¼ë¯¸í„° ìŠ¤ì¼€ì¼ë§"""
    ratio = data_size / base_size
    return {
        'min_cluster_size': max(30, int(50 * ratio)),
        'min_samples': max(5, int(10 * ratio)),
        'n_neighbors': max(15, min(30, int(25 * np.sqrt(ratio))))
    }

def smart_tokenizer(text):
    """ìŠ¤ë§ˆíŠ¸ í† í¬ë‚˜ì´ì €"""
    pattern = r'\b[ê°€-í£]{2,}\b|\b[A-Z]{2,}\b|\b[a-z]{3,}\b'
    tokens = re.findall(pattern, text.lower())
    filtered = []
    for token in tokens:
        if any(char.isdigit() for char in token):
            continue
        if len(token) < 2:
            continue
        filtered.append(token)
    return filtered

# ============================================================================
# ì‹œê°í™” í•¨ìˆ˜
# ============================================================================
def create_topic_distribution_chart(topics):
    """í† í”½ ë¶„í¬ ì°¨íŠ¸"""
    topic_counts = pd.Series(topics).value_counts().sort_index()
    topic_counts = topic_counts[topic_counts.index != -1]  # Outlier ì œì™¸
    
    fig = go.Figure(data=[
        go.Bar(
            x=topic_counts.index,
            y=topic_counts.values,
            text=topic_counts.values,
            textposition='auto',
            marker_color='#1565C0'
        )
    ])
    
    fig.update_layout(
        title='í† í”½ë³„ ë¬¸ì„œ ìˆ˜ (Outlier ì œì™¸)',
        xaxis_title='í† í”½ ë²ˆí˜¸',
        yaxis_title='ë¬¸ì„œ ìˆ˜',
        height=400
    )
    
    return fig

def create_outlier_chart(topics):
    """Outlier vs í† í”½ í• ë‹¹ ë¹„ìœ¨"""
    outlier_count = (topics == -1).sum()
    topic_count = (topics != -1).sum()
    
    fig = go.Figure(data=[
        go.Pie(
            labels=['í† í”½ í• ë‹¹', 'Outlier'],
            values=[topic_count, outlier_count],
            marker_colors=['#0D47A1','#1565C0'],
            hole=0.4
        )
    ])
    
    fig.update_layout(
        title='í† í”½ í• ë‹¹ vs Outlier',
        height=400
    )
    
    return fig

def create_keywords_table(topic_model, topics):
    """í† í”½ë³„ í‚¤ì›Œë“œ í…Œì´ë¸”"""
    unique_topics = sorted([t for t in set(topics) if t != -1])
    
    data = []
    for topic_id in unique_topics:
        count = (topics == topic_id).sum()
        pct = count / len(topics) * 100
        words = topic_model.get_topic(topic_id)
        
        if words:
            keywords = ', '.join([f"{w[0]}({w[1]:.3f})" for w in words[:5]])
            data.append({
                'í† í”½': f"Topic {topic_id}",
                'ë¬¸ì„œ ìˆ˜': f"{count:,}",
                'ë¹„ìœ¨': f"{pct:.1f}%",
                'ì£¼ìš” í‚¤ì›Œë“œ': keywords
            })
    
    return pd.DataFrame(data)

# ============================================================================
# ë©”ì¸ ì•±
# ============================================================================
def main():
    # í—¤ë”
    st.markdown('<div class="main-header">BERTopic í† í”½ ëª¨ë¸ë§</div>', unsafe_allow_html=True)
    st.markdown("---")
    
    # ============================================================================
    # 1. íŒŒì¼ ì—…ë¡œë“œ
    # ============================================================================
    st.markdown('<div class="sub-header">ğŸ“ 1. ë°ì´í„° ì—…ë¡œë“œ</div>', unsafe_allow_html=True)
    
    uploaded_file = st.file_uploader(
        "CSV íŒŒì¼ ì—…ë¡œë“œ (sentence ì»¬ëŸ¼ í•„ìˆ˜)",
        type=['csv'],
        help="BERTopic í† í”½ ëª¨ë¸ë§ì„ ìˆ˜í–‰í•  CSV íŒŒì¼"
    )
    
    if uploaded_file is None:
        st.info("â¬†ï¸ CSV íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
        return
    
    # ë°ì´í„° ë¡œë“œ
    try:
        df = pd.read_csv(uploaded_file)
        st.markdown(f"""
        <div style="background-color: #F0F2F6; padding: 1rem; border-radius: 0.5rem; margin-bottom: 1rem;">
            âœ… <strong>ë°ì´í„° ë¡œë“œ ì™„ë£Œ:</strong> {len(df):,}ê°œ ë¬¸ì„œ
        </div>
        """, unsafe_allow_html=True)
        
        if 'sentence' not in df.columns:
            st.error("âŒ 'sentence' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        texts = df['sentence'].tolist()
        
    except Exception as e:
        st.error(f"âŒ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return
    
    st.markdown("---")
    
    # ============================================================================
    # 2. íŒŒë¼ë¯¸í„° ì„¤ì •
    # ============================================================================
    st.markdown('<div class="sub-header">âš™ï¸ 2. íŒŒë¼ë¯¸í„° ì„¤ì •</div>', unsafe_allow_html=True)
    
    # ì„ë² ë”© ëª¨ë¸
    st.markdown("**ì„ë² ë”© ëª¨ë¸**")
    col1, col2 = st.columns([2, 1])
    
    with col1:
        embedding_model_key = st.selectbox(
            "í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸ ì„ íƒ",
            options=list(EMBEDDING_MODELS.keys()),
            format_func=lambda x: EMBEDDING_MODELS[x],
            index=0
        )
    
    with col2:
        use_embedding_cache = st.checkbox(
            "ì„ë² ë”© ìºì‹œ ì‚¬ìš©",
            value=True,
            help="ì´ì „ ìƒì„±ëœ ì„ë² ë”© ì¬ì‚¬ìš© (ê°™ì€ ì„¸ì…˜ ë‚´)"
        )
    
    st.markdown("---")
    
    # UMAP íŒŒë¼ë¯¸í„°
    st.markdown("**UMAP íŒŒë¼ë¯¸í„°**")
    col1, col2, col3 = st.columns(3)
    
    with col1:
        n_components = st.slider(
            "ì°¨ì› ìˆ˜ (n_components)",
            min_value=2,
            max_value=10,
            value=5,
            help="ì°¨ì› ì¶•ì†Œ í›„ ì°¨ì› ìˆ˜"
        )
    
    with col2:
        n_neighbors = st.slider(
            "ì´ì›ƒ ìˆ˜ (n_neighbors)",
            min_value=5,
            max_value=50,
            value=15,
            help="ë¡œì»¬ êµ¬ì¡° í•™ìŠµì— ì‚¬ìš©í•  ì´ì›ƒ ìˆ˜"
        )
    
    with col3:
        min_dist = st.slider(
            "ìµœì†Œ ê±°ë¦¬ (min_dist)",
            min_value=0.0,
            max_value=0.5,
            value=0.0,
            step=0.05,
            help="ì„ë² ë”© ê³µê°„ì—ì„œ ì ë“¤ ê°„ ìµœì†Œ ê±°ë¦¬"
        )
    
    st.markdown("---")
    
    # HDBSCAN íŒŒë¼ë¯¸í„°
    st.markdown("**HDBSCAN íŒŒë¼ë¯¸í„°**")
    col1, col2 = st.columns(2)
    
    with col1:
        min_cluster_size = st.slider(
            "ìµœì†Œ í´ëŸ¬ìŠ¤í„° í¬ê¸°",
            min_value=10,
            max_value=200,
            value=50,
            help="í´ëŸ¬ìŠ¤í„°ë¡œ ì¸ì •ë˜ê¸° ìœ„í•œ ìµœì†Œ ë¬¸ì„œ ìˆ˜"
        )
    
    with col2:
        min_samples = st.slider(
            "ìµœì†Œ ìƒ˜í”Œ ìˆ˜",
            min_value=1,
            max_value=50,
            value=10,
            help="ì½”ì–´ í¬ì¸íŠ¸ê°€ ë˜ê¸° ìœ„í•œ ìµœì†Œ ì´ì›ƒ ìˆ˜"
        )
    
    st.markdown("---")
    
    # í† í”½ ê°œìˆ˜ ì„¤ì •
    st.markdown("**í† í”½ ê°œìˆ˜ ì„¤ì •**")
    col1, col2 = st.columns(2)
    
    with col1:
        topic_mode = st.radio(
            "í† í”½ ê°œìˆ˜ ê²°ì • ë°©ì‹",
            options=['ìë™', 'ìˆ˜ë™'],
            index=0,
            horizontal=True,
            help="ìë™: ìë™ìœ¼ë¡œ ìµœì  ê°œìˆ˜ ê²°ì •, ìˆ˜ë™: ì§ì ‘ ê°œìˆ˜ ì§€ì •"
        )
    
    with col2:
        if topic_mode == 'ìˆ˜ë™':
            nr_topics = st.number_input(
                "í† í”½ ê°œìˆ˜",
                min_value=5,
                max_value=100,
                value=20,
                step=5
            )
        else:
            nr_topics = 'auto'
            st.info("ìë™ìœ¼ë¡œ ìµœì  í† í”½ ê°œìˆ˜ë¥¼ ê²°ì •í•©ë‹ˆë‹¤")
    
    st.markdown("---")
    
    # Vectorizer íŒŒë¼ë¯¸í„°
    st.markdown("**Vectorizer íŒŒë¼ë¯¸í„°**")
    col1, col2, col3 = st.columns(3)
    
    with col1:
        max_features = st.slider(
            "ìµœëŒ€ ë‹¨ì–´ ìˆ˜",
            min_value=50,
            max_value=500,
            value=200,
            step=50,
            help="í† í”½ í‘œí˜„ì— ì‚¬ìš©í•  ìµœëŒ€ ë‹¨ì–´ ê°œìˆ˜"
        )
    
    with col2:
        max_df = st.slider(
            "ìµœëŒ€ ë¬¸ì„œ ë¹ˆë„",
            min_value=0.5,
            max_value=1.0,
            value=0.8,
            step=0.1,
            help="ë„ˆë¬´ ìì£¼ ë‚˜íƒ€ë‚˜ëŠ” ë‹¨ì–´ ì œì™¸"
        )
    
    with col3:
        ngram_max = st.selectbox(
            "N-gram ìµœëŒ€ê°’",
            options=[1, 2, 3],
            index=0,
            help="1: ë‹¨ì¼ ë‹¨ì–´ë§Œ, 2: 2ë‹¨ì–´ ì¡°í•© í¬í•¨"
        )
    
    st.markdown("---")
    
    # ìƒ˜í”Œë§ ì˜µì…˜
    st.markdown("**ìƒ˜í”Œë§ ì˜µì…˜ (ëŒ€ìš©ëŸ‰ ë°ì´í„°ìš©)**")
    col1, col2 = st.columns(2)
    
    with col1:
        use_sampling = st.checkbox(
            "ìƒ˜í”Œë§ ì‚¬ìš©",
            value=len(df) > 50000,
            help="ë°ì´í„°ê°€ ë§ì„ ë•Œ ìƒ˜í”Œë¡œ í•™ìŠµ í›„ ì „ì²´ ì˜ˆì¸¡"
        )
    
    with col2:
        if use_sampling:
            sample_size = st.number_input(
                "ìƒ˜í”Œ í¬ê¸°",
                min_value=1000,
                max_value=min(100000, len(df)),
                value=min(50000, len(df)),
                step=5000
            )
        else:
            sample_size = len(df)
    
    # í˜„ì¬ ì„¤ì • ìš”ì•½
    with st.expander("ğŸ“‹ í˜„ì¬ ì„¤ì • ìš”ì•½"):
        st.write(f"""
        **ì„ë² ë”©**
        - ëª¨ë¸: {EMBEDDING_MODELS[embedding_model_key]}
        - ìºì‹œ ì‚¬ìš©: {'ì˜ˆ' if use_embedding_cache else 'ì•„ë‹ˆì˜¤'}
        
        **UMAP**
        - n_components: {n_components}
        - n_neighbors: {n_neighbors}
        - min_dist: {min_dist}
        
        **HDBSCAN**
        - min_cluster_size: {min_cluster_size}
        - min_samples: {min_samples}
        
        **í† í”½**
        - ê²°ì • ë°©ì‹: {topic_mode}
        - í† í”½ ê°œìˆ˜: {nr_topics if topic_mode == 'ìˆ˜ë™' else 'ìë™'}
        
        **Vectorizer**
        - max_features: {max_features}
        - max_df: {max_df}
        - ngram: (1, {ngram_max})
        
        **ìƒ˜í”Œë§**
        - ì‚¬ìš©: {'ì˜ˆ' if use_sampling else 'ì•„ë‹ˆì˜¤'}
        - í¬ê¸°: {sample_size:,}ê°œ / {len(df):,}ê°œ
        """)
    
    st.markdown("---")
    
    # ============================================================================
    # 3. í•™ìŠµ ì‹¤í–‰
    # ============================================================================
    if st.button("ğŸš€ BERTopic í•™ìŠµ ì‹œì‘", type="primary", use_container_width=True):
        start_time = time.time()
        
        try:
            # 1. ì„ë² ë”© ìƒì„±
            st.markdown("### 1ï¸âƒ£ ì„ë² ë”© ìƒì„±")
            
            cache_key = f"embeddings_{embedding_model_key}_{len(texts)}"
            
            if use_embedding_cache and cache_key in st.session_state:
                st.info("âœ… ìºì‹œëœ ì„ë² ë”© ì‚¬ìš©")
                embeddings = st.session_state[cache_key]
            else:
                # Progress bar ì¶”ê°€
                progress_bar = st.progress(0)
                status_text = st.empty()
                
                status_text.text(f"ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘... ({EMBEDDING_MODELS[embedding_model_key]})")
                progress_bar.progress(10)
                
                model = SentenceTransformer(embedding_model_key)
                
                progress_bar.progress(30)
                status_text.text(f"ì„ë² ë”© ìƒì„± ì¤‘... ({len(texts):,}ê°œ ë¬¸ì„œ)")
                
                # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì„ë² ë”© ìƒì„± + ì§„í–‰ë¥  í‘œì‹œ
                batch_size = 32
                embeddings_list = []
                
                for i in range(0, len(texts), batch_size):
                    batch = texts[i:i+batch_size]
                    batch_embeddings = model.encode(
                        batch,
                        batch_size=batch_size,
                        show_progress_bar=False,
                        convert_to_numpy=True
                    )
                    embeddings_list.append(batch_embeddings)
                    
                    # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ (30% ~ 90%)
                    progress = 30 + int((i / len(texts)) * 60)
                    progress_bar.progress(min(progress, 90))
                    status_text.text(f"ì„ë² ë”© ìƒì„± ì¤‘... {i+len(batch):,}/{len(texts):,} ({progress}%)")
                
                embeddings = np.vstack(embeddings_list)
                st.session_state[cache_key] = embeddings
                
                progress_bar.progress(100)
                status_text.text(f"âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ!")
                
                st.markdown(f"""
                <div style="background-color: #F0F2F6; padding: 1rem; border-radius: 0.5rem; margin-bottom: 1rem;">
                    âœ… <strong>ì„ë² ë”© ìƒì„± ì™„ë£Œ:</strong> {embeddings.shape}
                </div>
                """, unsafe_allow_html=True)
                
                # progress bar ì •ë¦¬
                time.sleep(0.5)
                progress_bar.empty()
                status_text.empty()
            
            # 2. ìƒ˜í”Œë§ (ì˜µì…˜)
            st.markdown("### 2ï¸âƒ£ ë°ì´í„° ì¤€ë¹„")
            
            if use_sampling and sample_size < len(texts):
                np.random.seed(42)
                sample_indices = np.random.choice(len(embeddings), sample_size, replace=False)
                train_embeddings = embeddings[sample_indices]
                train_texts = [texts[i] for i in sample_indices]
                st.info(f"ìƒ˜í”Œ ì‚¬ìš©: {sample_size:,}ê°œë¡œ í•™ìŠµ")
            else:
                train_embeddings = embeddings
                train_texts = texts
                st.info(f"ì „ì²´ ë°ì´í„° ì‚¬ìš©: {len(texts):,}ê°œ")
            
            # 3. BERTopic ëª¨ë¸ í•™ìŠµ
            st.markdown("### 3ï¸âƒ£ BERTopic ëª¨ë¸ í•™ìŠµ")
            
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            status_text.text("UMAP ì°¨ì› ì¶•ì†Œ ì¤€ë¹„ ì¤‘...")
            progress_bar.progress(20)
            
            # UMAP
            umap_model = UMAP(
                n_components=n_components,
                n_neighbors=n_neighbors,
                min_dist=min_dist,
                metric='cosine',
                random_state=42
            )
            
            status_text.text("HDBSCAN í´ëŸ¬ìŠ¤í„°ë§ ì¤€ë¹„ ì¤‘...")
            progress_bar.progress(40)
            
            # HDBSCAN
            hdbscan_model = HDBSCAN(
                min_cluster_size=min_cluster_size,
                min_samples=min_samples,
                cluster_selection_method='eom',
                metric='euclidean',
                prediction_data=False
            )
            
            status_text.text("Vectorizer ì¤€ë¹„ ì¤‘...")
            progress_bar.progress(50)
            
            # Vectorizer
            vectorizer_model = CountVectorizer(
                tokenizer=smart_tokenizer,
                max_features=max_features,
                max_df=max_df,
                ngram_range=(1, ngram_max)
            )
            
            status_text.text("BERTopic ëª¨ë¸ ìƒì„± ì¤‘...")
            progress_bar.progress(60)
            
            # BERTopic
            topic_model = BERTopic(
                umap_model=umap_model,
                hdbscan_model=hdbscan_model,
                vectorizer_model=vectorizer_model,
                nr_topics=nr_topics if topic_mode == 'ìˆ˜ë™' else 'auto',
                min_topic_size=max(10, int(len(texts) * 0.001)),
                calculate_probabilities=False,
                verbose=False
            )
            
            status_text.text("í† í”½ í•™ìŠµ ì¤‘... (UMAP + HDBSCAN + c-TF-IDF)")
            progress_bar.progress(70)
            
            topics, probs = topic_model.fit_transform(train_texts, train_embeddings)
            topics = np.array(topics)
            
            progress_bar.progress(100)
            status_text.text("âœ… í•™ìŠµ ì™„ë£Œ!")
            
            st.markdown("""
            <div style="background-color: #F0F2F6; padding: 1rem; border-radius: 0.5rem; margin-bottom: 1rem;">
                âœ… <strong>í•™ìŠµ ì™„ë£Œ!</strong>
            </div>
            """, unsafe_allow_html=True)
            
            # progress bar ì •ë¦¬
            time.sleep(0.5)
            progress_bar.empty()
            status_text.empty()
            
            # 4. ì „ì²´ ë°ì´í„° ì˜ˆì¸¡ (ìƒ˜í”Œë§ ì‚¬ìš© ì‹œ)
            if use_sampling and sample_size < len(texts):
                st.markdown("### 4ï¸âƒ£ ì „ì²´ ë°ì´í„° ì˜ˆì¸¡")
                with st.spinner("ì „ì²´ ë°ì´í„°ì— í† í”½ í• ë‹¹ ì¤‘..."):
                    topics, _ = topic_model.transform(texts, embeddings)
                    topics = np.array(topics)
                st.markdown("""
                <div style="background-color: #F0F2F6; padding: 1rem; border-radius: 0.5rem; margin-bottom: 1rem;">
                    âœ… <strong>ì˜ˆì¸¡ ì™„ë£Œ!</strong>
                </div>
                """, unsafe_allow_html=True)
            
            # ê²°ê³¼ ì €ì¥
            st.session_state['topic_model'] = topic_model
            st.session_state['topics'] = topics
            st.session_state['df_result'] = df.copy()
            st.session_state['df_result']['bertopic_topic'] = topics
            st.session_state['df_result']['outlier'] = (topics == -1).astype(int)
            
            elapsed = time.time() - start_time
            st.markdown(f"""
            <div style="background-color: #F0F2F6; padding: 1rem; border-radius: 0.5rem; margin-bottom: 1rem;">
                ğŸ‰ <strong>ì „ì²´ ì™„ë£Œ!</strong> (ì´ ì†Œìš” ì‹œê°„: {elapsed/60:.1f}ë¶„)
            </div>
            """, unsafe_allow_html=True)
            
        except Exception as e:
            st.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            st.text(traceback.format_exc())
            return
        
        st.markdown("---")
    
    # ============================================================================
    # 4. ê²°ê³¼ ì¶œë ¥
    # ============================================================================
    if 'topics' in st.session_state:
        topics = st.session_state['topics']
        topic_model = st.session_state['topic_model']
        df_result = st.session_state['df_result']
        
        st.markdown('<div class="sub-header">ğŸ“Š 3. í•™ìŠµ ê²°ê³¼</div>', unsafe_allow_html=True)
        
        # ì£¼ìš” í†µê³„
        outlier_count = (topics == -1).sum()
        outlier_pct = outlier_count / len(topics) * 100
        unique_topics = sorted([t for t in set(topics) if t != -1])
        n_topics = len(unique_topics)
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("ì „ì²´ ë¬¸ì„œ", f"{len(topics):,}")
        
        with col2:
            st.metric("í† í”½ ìˆ˜", n_topics)
        
        with col3:
            st.metric("Outlier", f"{outlier_count:,} ({outlier_pct:.1f}%)")
        
        with col4:
            # í‰ê°€
            if outlier_pct < 25:
                status = "ìš°ìˆ˜"
            elif outlier_pct < 35:
                status = "ì–‘í˜¸"
            elif outlier_pct < 45:
                status = "ë³´í†µ"
            else:
                status = "ê°œì„ í•„ìš”"
            st.metric("í‰ê°€", status)
        
        st.markdown("---")
        
        # ì°¨íŠ¸
        col1, col2 = st.columns(2)
        
        with col1:
            fig1 = create_topic_distribution_chart(topics)
            st.plotly_chart(fig1, use_container_width=True)
        
        with col2:
            fig2 = create_outlier_chart(topics)
            st.plotly_chart(fig2, use_container_width=True)
        
        st.markdown("---")
        
        # í† í”½ë³„ í‚¤ì›Œë“œ
        st.markdown("**í† í”½ë³„ ì£¼ìš” í‚¤ì›Œë“œ**")
        keywords_df = create_keywords_table(topic_model, topics)
        st.dataframe(keywords_df, use_container_width=True)
        
        st.markdown("---")
        
        # ============================================================================
        # í† í”½ ì„ íƒ ë° í•„í„°ë§
        # ============================================================================
        st.markdown('<div class="sub-header">ğŸ¯ í† í”½ ì„ íƒ ë° í•„í„°ë§</div>', unsafe_allow_html=True)
        
        st.write("**ë¶„ì„í•  í† í”½ì„ ì„ íƒí•˜ì„¸ìš”** (ê°ì„±ë¶„ì„/íšŒê·€ë¶„ì„ ë“± í›„ì† ë¶„ì„ìš©)")
        
        # í† í”½ë³„ ì •ë³´ë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë§Œë“¤ê¸°
        topic_info = []
        for topic_id in unique_topics:
            count = (topics == topic_id).sum()
            pct = count / len(topics) * 100
            words = topic_model.get_topic(topic_id)
            if words:
                keywords = ', '.join([w[0] for w in words[:5]])
                topic_info.append({
                    'Topic ID': topic_id,
                    'ë¬¸ì„œ ìˆ˜': count,
                    'ë¹„ìœ¨ (%)': f"{pct:.1f}",
                    'ì£¼ìš” í‚¤ì›Œë“œ': keywords
                })
        
        topic_info_df = pd.DataFrame(topic_info)
        
        # í† í”½ ì •ë³´ í‘œì‹œ
        st.dataframe(topic_info_df, use_container_width=True, height=300)
        
        # Outlier í¬í•¨ ì—¬ë¶€
        include_outlier = st.checkbox(
            "Outlier (-1) í¬í•¨",
            value=False,
            help="ì²´í¬í•˜ë©´ Outlier í† í”½ë„ ê²°ê³¼ì— í¬í•¨ë©ë‹ˆë‹¤"
        )
        
        # í† í”½ ì„ íƒ UI
        col1, col2 = st.columns([3, 1])
        
        # ì„ íƒ ê°€ëŠ¥í•œ í† í”½ ëª©ë¡
        available_topics = unique_topics.copy()
        if include_outlier:
            available_topics = [-1] + available_topics
        
        # session_state ì´ˆê¸°í™”
        if 'selected_bertopic_list' not in st.session_state:
            st.session_state['selected_bertopic_list'] = unique_topics[:min(3, len(unique_topics))]
        
        with col2:
            if st.button("ğŸ”„ ì „ì²´ ì„ íƒ", key="select_all", use_container_width=True):
                st.session_state['selected_bertopic_list'] = available_topics
                st.rerun()
            
            if st.button("âŒ ì „ì²´ í•´ì œ", key="clear_all", use_container_width=True):
                st.session_state['selected_bertopic_list'] = []
                st.rerun()
        
        with col1:
            selected_topics = st.multiselect(
                "ë¶„ì„í•  í† í”½ ì„ íƒ",
                options=available_topics,
                default=st.session_state['selected_bertopic_list'],
                help="ì—¬ëŸ¬ ê°œ ì„ íƒ ê°€ëŠ¥í•©ë‹ˆë‹¤. ì„ íƒí•œ í† í”½ë§Œ í•„í„°ë§í•˜ì—¬ ì €ì¥ë©ë‹ˆë‹¤.",
                format_func=lambda x: f"Topic {x}" if x != -1 else "Outlier (-1)"
            )
            
            # multiselect ê°’ì´ ë³€ê²½ë˜ë©´ session_state ì—…ë°ì´íŠ¸
            st.session_state['selected_bertopic_list'] = selected_topics
        
        # ì„ íƒ ê²°ê³¼ í‘œì‹œ
        if selected_topics:
            filtered_df = df_result[df_result['bertopic_topic'].isin(selected_topics)].copy()
            
            st.markdown(f"""
            <div style="background-color: #F0F2F6; padding: 1rem; border-radius: 0.5rem; margin-bottom: 1rem;">
                âœ… <strong>{len(selected_topics)}ê°œ í† í”½ ì„ íƒë¨</strong> (ì´ {len(filtered_df):,}ê°œ ë¬¸ì„œ)
            </div>
            """, unsafe_allow_html=True)
            
            # ì„ íƒí•œ í† í”½ ìš”ì•½
            with st.expander("ğŸ“Š ì„ íƒí•œ í† í”½ ìš”ì•½"):
                for topic_id in selected_topics:
                    count = (filtered_df['bertopic_topic'] == topic_id).sum()
                    pct = count / len(filtered_df) * 100
                    
                    if topic_id == -1:
                        st.write(f"**Outlier (-1)** ({count:,}ê°œ, {pct:.1f}%): ë¯¸ë¶„ë¥˜ ë¬¸ì„œ")
                    else:
                        words = topic_model.get_topic(topic_id)
                        if words:
                            keywords = ', '.join([f"{w[0]}({w[1]:.3f})" for w in words[:5]])
                            st.write(f"**Topic {topic_id}** ({count:,}ê°œ, {pct:.1f}%): {keywords}")
            
            # í† í”½ë³„ ìƒì„¸ ì •ë³´ (LDA ìŠ¤íƒ€ì¼)
            with st.expander("ğŸ” í† í”½ë³„ ìƒì„¸ ì •ë³´"):
                for topic_id in unique_topics[:20]:  # ìƒìœ„ 20ê°œë§Œ
                    count = (topics == topic_id).sum()
                    pct = count / len(topics) * 100
                    words = topic_model.get_topic(topic_id)
                    
                    if words:
                        keywords = ', '.join([f"{w[0]}({w[1]:.3f})" for w in words[:10]])
                        
                        # ì„ íƒëœ í† í”½ ê°•ì¡°
                        if topic_id in selected_topics:
                            st.markdown(f"**âœ… Topic {topic_id}** ({count:,}ê°œ ë¬¸ì„œ, {pct:.1f}%) - **ì„ íƒë¨**")
                        else:
                            st.markdown(f"**Topic {topic_id}** ({count:,}ê°œ ë¬¸ì„œ, {pct:.1f}%)")
                        
                        st.text(keywords)
                        st.markdown("---")
            
            # ì„¸ì…˜ì— ì €ì¥ (ë‹¤ë¥¸ ë¶„ì„ì—ì„œ ì‚¬ìš© ê°€ëŠ¥)
            st.session_state['filtered_df'] = filtered_df
            st.session_state['selected_topics'] = selected_topics
            
            # ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° (ì„ íƒí•œ í† í”½ë§Œ)
            with st.expander("ğŸ“„ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ 100ê°œ)"):
                display_cols = ['sentence', 'bertopic_topic', 'outlier']
                if 'company' in filtered_df.columns:
                    display_cols.insert(1, 'company')
                if 'label' in filtered_df.columns:
                    display_cols.insert(2, 'label')
                
                display_cols = [col for col in display_cols if col in filtered_df.columns]
                st.dataframe(filtered_df[display_cols].head(100), use_container_width=True)
            
        else:
            st.warning("âš ï¸ ìµœì†Œ 1ê°œ ì´ìƒì˜ í† í”½ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
            filtered_df = df_result
        
        st.markdown("---")
        
        # ============================================================================
        # 5. ê²°ê³¼ ì €ì¥
        # ============================================================================
        st.markdown('<div class="sub-header">ğŸ’¾ 4. ê²°ê³¼ ì €ì¥</div>', unsafe_allow_html=True)
        
        st.info(f"ğŸ’¡ **ì„ íƒí•œ í† í”½ ({len(selected_topics)}ê°œ)ì˜ ë°ì´í„°ë§Œ ì €ì¥ë©ë‹ˆë‹¤** ({len(filtered_df):,}ê°œ ë¬¸ì„œ)")
        
        col1, col2, col3 = st.columns(3)
        
        # CSV ì €ì¥
        with col1:
            st.write("**ğŸ’¾ CSV ì €ì¥**")
            
            default_path = str(Path.home() / "Desktop" / f"bertopic_result_selected_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv")
            
            save_path = st.text_input(
                "ì €ì¥ ê²½ë¡œ",
                value=default_path,
                help="íŒŒì¼ì„ ì €ì¥í•  ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”",
                key="csv_path"
            )
            
            if st.button("ğŸ’¾ íŒŒì¼ë¡œ ì €ì¥", key="save_csv", use_container_width=True):
                try:
                    filtered_df.to_csv(save_path, index=False, encoding='utf-8-sig')
                    st.markdown(f"""
                    <div style="background-color: #F0F2F6; padding: 1rem; border-radius: 0.5rem; margin-bottom: 1rem;">
                        âœ… <strong>ì €ì¥ ì™„ë£Œ!</strong><br>{save_path}
                    </div>
                    """, unsafe_allow_html=True)
                    
                    # íŒŒì¼ í¬ê¸° í‘œì‹œ
                    import os
                    file_size = os.path.getsize(save_path) / 1024
                    st.info(f"ğŸ“Š íŒŒì¼ í¬ê¸°: {file_size:.2f} KB")
                    
                except Exception as e:
                    st.error(f"âŒ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
            
            st.caption(f"ğŸ’¡ ì„ íƒí•œ í† í”½: {len(selected_topics)}ê°œ\në¬¸ì„œ: {len(filtered_df):,}ê°œ")
        
        # Excel ì €ì¥
        with col2:
            st.write("**ğŸ’¾ Excel ì €ì¥**")
            
            default_path_excel = str(Path.home() / "Desktop" / f"bertopic_result_selected_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx")
            
            save_path_excel = st.text_input(
                "ì €ì¥ ê²½ë¡œ (Excel)",
                value=default_path_excel,
                help="Excel íŒŒì¼ì„ ì €ì¥í•  ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”",
                key="excel_path"
            )
            
            if st.button("ğŸ’¾ Excelë¡œ ì €ì¥", key="save_excel", use_container_width=True):
                try:
                    with pd.ExcelWriter(save_path_excel, engine='openpyxl') as writer:
                        filtered_df.to_excel(writer, index=False, sheet_name='ì„ íƒí•œí† í”½')
                        keywords_df.to_excel(writer, index=False, sheet_name='ì „ì²´í† í”½í‚¤ì›Œë“œ')
                        
                        # ì„ íƒí•œ í† í”½ ì •ë³´ ì‹œíŠ¸ ì¶”ê°€
                        selected_info = topic_info_df[topic_info_df['Topic ID'].isin(selected_topics)]
                        selected_info.to_excel(writer, index=False, sheet_name='ì„ íƒí•œí† í”½ì •ë³´')
                    
                    st.markdown(f"""
                    <div style="background-color: #F0F2F6; padding: 1rem; border-radius: 0.5rem; margin-bottom: 1rem;">
                        âœ… <strong>ì €ì¥ ì™„ë£Œ!</strong><br>{save_path_excel}
                    </div>
                    """, unsafe_allow_html=True)
                    
                    import os
                    file_size = os.path.getsize(save_path_excel) / 1024
                    st.info(f"ğŸ“Š íŒŒì¼ í¬ê¸°: {file_size:.2f} KB")
                    
                except Exception as e:
                    st.error(f"âŒ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
            
            st.caption("ğŸ’¡ 3ê°œ ì‹œíŠ¸ í¬í•¨\n(ì„ íƒí•œí† í”½, ì „ì²´í† í”½í‚¤ì›Œë“œ, ì„ íƒí•œí† í”½ì •ë³´)")
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        with col3:
            st.write("**ğŸ’¾ ë©”íƒ€ë°ì´í„° ì €ì¥**")
            
            default_path_json = str(Path.home() / "Desktop" / f"bertopic_metadata_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
            
            save_path_json = st.text_input(
                "ì €ì¥ ê²½ë¡œ (JSON)",
                value=default_path_json,
                help="ë©”íƒ€ë°ì´í„° JSON íŒŒì¼ì„ ì €ì¥í•  ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”",
                key="json_path"
            )
            
            if st.button("ğŸ’¾ JSONìœ¼ë¡œ ì €ì¥", key="save_json", use_container_width=True):
                try:
                    metadata = {
                        'n_topics': n_topics,
                        'selected_topics': [int(t) for t in selected_topics],
                        'filtered_documents': len(filtered_df),
                        'outlier_count': int(outlier_count),
                        'outlier_percentage': float(outlier_pct),
                        'total_documents': len(topics),
                        'parameters': {
                            'embedding_model': embedding_model_key,
                            'n_components': n_components,
                            'n_neighbors': n_neighbors,
                            'min_dist': min_dist,
                            'min_cluster_size': min_cluster_size,
                            'min_samples': min_samples,
                            'topic_mode': topic_mode,
                            'nr_topics': nr_topics if topic_mode == 'ìˆ˜ë™' else 'auto',
                            'max_features': max_features,
                            'max_df': max_df,
                            'ngram_range': f"(1, {ngram_max})"
                        },
                        'timestamp': datetime.now().isoformat()
                    }
                    
                    with open(save_path_json, 'w', encoding='utf-8') as f:
                        json.dump(metadata, f, ensure_ascii=False, indent=2)
                    
                    st.markdown(f"""
                    <div style="background-color: #F0F2F6; padding: 1rem; border-radius: 0.5rem; margin-bottom: 1rem;">
                        âœ… <strong>ì €ì¥ ì™„ë£Œ!</strong><br>{save_path_json}
                    </div>
                    """, unsafe_allow_html=True)
                    
                    import os
                    file_size = os.path.getsize(save_path_json) / 1024
                    st.info(f"ğŸ“Š íŒŒì¼ í¬ê¸°: {file_size:.2f} KB")
                    
                except Exception as e:
                    st.error(f"âŒ ì €ì¥ ì‹¤íŒ¨: {str(e)}")

if __name__ == "__main__":
    main()